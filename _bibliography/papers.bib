---
---

@article{animatediff2024,
  abbr={ICLR},
  title={AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning},
  author={Guo*, Yuwei and Yang*, Lichao and Li, Chen and Chen, Zhongang and Liu, Jumeng and Shan, Yin and Wang, Xin},
  abstract={Recent text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images. However, extending these models to video generation while maintaining temporal consistency and motion coherence remains challenging. We present AnimateDiff, a novel approach that transforms existing text-to-image models into text-to-video ones without requiring specific tuning. Our method introduces motion modeling through a simple yet effective temporal layer design, enabling the generation of animated content from static image generators. The approach maintains the original model's personalization capabilities while adding fluid motion generation. We demonstrate AnimateDiff's effectiveness across various scenarios including character animation, dynamic scene generation, and style-consistent video creation.},
  journal={arXiv preprint arXiv:2307.04725},
  year={2023},
  month={July},
  publisher={International Conference on Learning Representations},
  arxiv={2307.04725},
  website={https://animatediff.github.io/},
  code={https://github.com/guoyww/AnimateDiff},
  video={https://www.youtube.com/embed/4c33yVVxvlM},
  preview={brownian-motion.gif},
  selected={true},
  dimensions={true},
  altmetric={true},
  google_scholar_id={HtK9qgF5KFIC},
  additional_info={. Project page includes interactive demos and additional results},
  annotation={* Equal contribution}
}
