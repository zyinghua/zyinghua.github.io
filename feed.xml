<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://zyinghua.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://zyinghua.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-15T11:10:37+00:00</updated><id>https://zyinghua.github.io/feed.xml</id><title type="html">blank</title><subtitle>Yinghua Zhou&apos;s site. </subtitle><entry><title type="html">Git access with VPN proxy</title><link href="https://zyinghua.github.io/blog/2025/git-access-with-vpn-proxy/" rel="alternate" type="text/html" title="Git access with VPN proxy"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2025/git-access-with-vpn-proxy</id><content type="html" xml:base="https://zyinghua.github.io/blog/2025/git-access-with-vpn-proxy/"><![CDATA[<h3 id="background">Background:</h3> <p>Github operations like clone, pull, push are sometimes restricted in regions including China, posting significant inconvinience. This post shares a solution to this problem, assuming you have an access to a VPN in your region, and you are using a Windows system (Mac users may refer to a similar set of steps with slightly different commands).</p> <hr/> <h3 id="solution">Solution:</h3> <p><br/></p> <h4 id="step-1">Step &lt;1&gt;</h4> <p>We need to find the socks5 proxy of the VPN. We can firstly check the relevant connections via <code class="language-plaintext highlighter-rouge">netstat -ano | findstr LISTENING</code> in cmd, then scroll down to find the TCP connections with port (2nd column) formulated as <code class="language-plaintext highlighter-rouge">127.0.0.1:&lt;port no.&gt;</code>.</p> <h4 id="step-2">Step &lt;2&gt;</h4> <p>Then, check the applications using the connection PID (4th column) via <code class="language-plaintext highlighter-rouge">tasklist /FI "PID eq &lt;PID&gt;"</code>, if it shows the name of your VPN, then it is the one.</p> <h4 id="step-3">Step &lt;3&gt;</h4> <p>You may have multiple ports associated with the same PID, we need to find which one is the socks5 proxy port to use. We can do so by verifying the port via <code class="language-plaintext highlighter-rouge">curl --proxy socks5h://127.0.0.1:&lt;port no.&gt; https://www.google.com</code>. If it returns a html string, then it is the one. Otherwise, it may return errors or just stuck.</p> <h4 id="step-4">Step &lt;4&gt;</h4> <p>Now, use the port no. in command line as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git config --global http.proxy "socks5h://127.0.0.1:&lt;port no.&gt;"
git config --global https.proxy "socks5h://127.0.0.1:&lt;port no.&gt;"
</code></pre></div></div> <p>Hopefully it solves your git issues!</p>]]></content><author><name></name></author><category term="solutions"/><category term="git"/><category term="git"/><category term="git-config"/><category term="proxy"/><category term="vpn"/><category term="china-git-solution"/><summary type="html"><![CDATA[A solution to inaccessibility of github in restricted regions including China]]></summary></entry><entry><title type="html">Understanding Diffusion Models</title><link href="https://zyinghua.github.io/blog/2024/understanding-diffusion-models/" rel="alternate" type="text/html" title="Understanding Diffusion Models"/><published>2024-06-27T00:00:00+00:00</published><updated>2024-06-27T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2024/understanding-diffusion-models</id><content type="html" xml:base="https://zyinghua.github.io/blog/2024/understanding-diffusion-models/"><![CDATA[<h3 id="behind-diffusion-models">Behind Diffusion Models</h3> <div class="complex-math"> Diffusion models were first introduced in the seminal work by [Sohl-Dickstein et al. (2015)](https://arxiv.org/abs/1503.03585). The core idea involves reversing a Markov chain-based forward diffusion process, which gradually degrades the structure of the data $\mathbf{z}_0$ from the real data distribution $q(\mathbf{z}_0)$, by adding noise over a sufficient number of steps. When this noise is Gaussian, as is commonly assumed in practice, the cumulative effect transforms the data distribution towards a standard normal distribution $\mathcal{N}(0, I)$ as the forward process progresses. We can then sample from this distribution and use a learned reverse process to generate new samples that match the real data distribution. Specifically, as illustrated in Figure 1, we define the forward diffusion process $q$ that gradually applies Gaussian noise to the data distribution $\mathbf{z}_0 \sim q(\mathbf{z}_0)$ with a variance schedule $\beta_t \in (0, 1)$ as follows: $$ q(\mathbf{z}_{1:T} \vert \mathbf{z}_0) := \prod_{t=1}^T q(\mathbf{z}_t \vert \mathbf{z}_{t-1}) \tag{1} $$ $$ q(\mathbf{z}_t \vert \mathbf{z}_{t-1}) := \mathcal{N}(\mathbf{z}_t; \sqrt{1 - \beta_t} \mathbf{z}_{t-1}, \beta_t \mathbf{I}) \tag{2} $$ <br/> <img src="/assets/img/post_images/dm-bg-illustration.png" alt="Illustration of the forward and reverse diffusion process. Figure adapted from Ho et al. (2020)." class="center" width="100%" style="max-width: 800px;"/> *Figure 1: Illustration of the forward and reverse diffusion process. Figure adapted from [Ho et al. (2020)](https://arxiv.org/abs/2006.11239).* Here, $t$ and $T$ refer to the current and total timesteps, respectively. With a sufficiently large $T$ and a properly defined variance schedule $\beta$, the final $\mathbf{z}_T$ will approximate an isotropic Gaussian distribution, where variances are equal in all directions. We can then reverse the forward diffusion process by starting from $\mathbf{z}_T \sim \mathcal{N}(0, I)$ and gradually obtain a sample $\mathbf{z}_0$ that resembles the real data distribution $q(\mathbf{z}_0)$, defined as follows: $$ p_{\theta}(\mathbf{z}_{0:T}) := p(\mathbf{z}_T) \prod_{t=1}^{T} p_{\theta}(\mathbf{z}_{t-1} \vert \mathbf{z}_{t}) \tag{3} $$ $$ p_{\theta}(\mathbf{z}_{t-1} \vert \mathbf{z}_{t}) := \mathcal{N}(\mathbf{z}_{t-1}; \mu_{\theta}(\mathbf{z}_{t}, t), \Sigma_{\theta}(\mathbf{z}_{t}, t)) \tag{4} $$ Where all $\mathbf{z}_t$ for $t \in \{0, 1, \ldots, T \}$ share the same dimensionality, $p_{\theta}(\mathbf{z}_{t-1} \vert \mathbf{z}_{t})$ is practically a neural network learned to approximate the actual reverse distribution $q(\mathbf{z}_{t-1} \vert \mathbf{z}_{t})$, as it depends on the entire dataset which is not easily accessible. The term $p_{\theta}(\mathbf{z}_{0:T})$ denotes the entire learned reverse process. An interesting property of the forward process is the existence of a closed-form sampling solution for $\mathbf{z}_t$ at an arbitrary $t \in [0, T]$ using the reparameterization trick. By defining $\alpha_t := 1 - \beta_t$ and $\bar{\alpha}_t := \prod_{i=1}^{t} \alpha_i$, given Equation (2), and recall that $\mathcal{N}(z; \mu, \sigma^2\mathbf{I})$ is equivalent to $z = \mu + \sigma \epsilon$ where $\epsilon \sim \mathcal{N}(0, 1)$, we obtain: $$ \begin{align} \mathbf{z}_t &amp;= \sqrt{1 - \beta_t} \mathbf{z}_{t-1} + \sqrt{\beta_t} \boldsymbol{\epsilon} \tag{5} \\ &amp;= \sqrt{\alpha_t} \mathbf{z}_{t-1} + \sqrt{1 - \alpha_t} \boldsymbol{\epsilon} \tag{6} \\ &amp;= \sqrt{\alpha_t \alpha_{t-1}} \mathbf{z}_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \boldsymbol{\epsilon} \tag{7} \\ &amp;= \sqrt{\alpha_t \alpha_{t-1} \alpha_{t-2}} \mathbf{z}_{t-3} + \sqrt{1 - \alpha_t \alpha_{t-1} \alpha_{t-2}} \boldsymbol{\epsilon} \tag{8} \\ &amp;\quad \vdots \notag \\ &amp;= \sqrt{\alpha_t \alpha_{t-1} \ldots \alpha_1 \alpha_0} \mathbf{z}_0 + \sqrt{1 - \alpha_t \alpha_{t-1} \ldots \alpha_1 \alpha_0} \boldsymbol{\epsilon} \tag{9} \\ &amp;= \sqrt{\bar{\alpha}_t} \mathbf{z}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon} \tag{10} \end{align} $$ Which leads to: $$ q(\mathbf{z}_t \vert \mathbf{z}_0) = \mathcal{N}(\mathbf{z}_t; \sqrt{\bar{\alpha}_t} \mathbf{z}_0, (1 - \bar{\alpha}_t) \mathbf{I}) \tag{11} $$ Recall that combining two Gaussian distributions with different variances, $\mathcal{N}(z; \mu, \sigma_1^2\mathbf{I})$ and $\mathcal{N}(z; \mu, \sigma_2^2\mathbf{I})$, results in $\mathcal{N}(z; \mu, (\sigma_1^2+\sigma_2^2)\mathbf{I})$. Thus, in the case of Equation (7), for transitioning from $t$ to $t-1$, the variance becomes $\sqrt{(1 - \alpha_t) + \alpha_t (1-\alpha_{t-1})} = \sqrt{1 - \alpha_t\alpha_{t-1}}$, with subsequent derivations following the same intuition. The goal is to learn a network $p_\theta$ that can approximate the actual reverse process. We start by looking at the objective that we want to minimize: the negative log-likelihood, $- \log p_\theta(\mathbf{z}_0)$, which involves maximizing the likelihood of the real data. However, directly optimizing this objective is practically infeasible due to its dependence on the entire sequence of previous steps. As a solution ([Sohl-Dickstein et al. (2015)](https://arxiv.org/abs/1503.03585)), we can derive an variational upper bound (vub) on the data log-likelihood to achieve the same optimization effect: $$ \begin{align} - \log p_\theta(\mathbf{z}_0) &amp;\leq - \log p_\theta(\mathbf{z}_0) + D_\text{KL}(q(\mathbf{z}_{1:T} \vert \mathbf{z}_0) \Vert p_\theta(\mathbf{z}_{1:T} \vert \mathbf{z}_0)) \tag{12} \\ &amp;= -\log p_\theta(\mathbf{z}_0) + \mathbb{E}_q \left[ \log \frac{q(\mathbf{z}_{1:T} \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_{1:T} \vert \mathbf{z}_0)} \right] \tag{13} \\ &amp;= -\log p_\theta(\mathbf{z}_0) + \mathbb{E}_q \left[ \log \frac{q(\mathbf{z}_{1:T} \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_{0}, \mathbf{z}_{1:T}) / p_\theta(\mathbf{z}_{0})} \right], \quad \text{using Bayes' rule} \tag{14} \\ &amp;= -\log p_\theta(\mathbf{z}_0) + \mathbb{E}_q \left[ \log \frac{q(\mathbf{z}_{1:T} \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_{0:T}) / p_\theta(\mathbf{z}_0)} \right] \tag{15} \\ &amp;= -\log p_\theta(\mathbf{z}_0) + \mathbb{E}_q \left[ \log \frac{q(\mathbf{z}_{1:T} \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_{0:T})} + \log p_\theta(\mathbf{z}_0) \right] \tag{16} \\ &amp;= \mathbb{E}_q \left[ \log \frac{q(\mathbf{z}_{1:T} \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_{0:T})} \right] := L_{vub} \tag{17} \end{align} $$ Note that the numerator and denominator of $L_{vub}$ refer to the forward process and the learned reverse process, respectively. Therefore, we can derive the following expressions ([Sohl-Dickstein et al. (2015)](https://arxiv.org/abs/1503.03585), [Ho et al. (2020)](https://arxiv.org/abs/2006.11239)): $$ \begin{align} L_\text{vub} &amp;= \mathbb{E}_{q} \Bigg[ \log\frac{q(\mathbf{z}_{1:T}\vert\mathbf{z}_0)}{p_\theta(\mathbf{z}_{0:T})} \Bigg] \tag{18} \\ &amp;= \mathbb{E}_q \Bigg[ \log\frac{\prod_{t=1}^T q(\mathbf{z}_t\vert\mathbf{z}_{t-1})}{ p(\mathbf{z}_T) \prod_{t=1}^T p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t) } \Bigg] \tag{19} \\ &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=1}^T \log \frac{q(\mathbf{z}_t\vert\mathbf{z}_{t-1})}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} \Bigg], \quad \text{using Logarithmic rules} \tag{20} \\ &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{z}_t\vert\mathbf{z}_{t-1})}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} + \log\frac{q(\mathbf{z}_1 \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)} \Bigg] \tag{21} \end{align} $$ Here, given the intrinsically high variance in the intermediate samples that introduces intractability in modeling the reverse process, we can condition on $\mathbf{z}_0$ due to its accessibility and feasibility to reduce the variance and make it tractable: $$ \begin{align} L_\text{vub} &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{z}_t\vert\mathbf{z}_{t-1})}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} + \log\frac{q(\mathbf{z}_1 \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)} \Bigg] \tag{22} \\ &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{z}_{t-1}\vert\mathbf{z}_{t}) q(\mathbf{z}_t)}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t) q(\mathbf{z}_{t-1})} + \log\frac{q(\mathbf{z}_1 \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)} \Bigg], \quad \text{using Bayes' rule} \tag{23} \\ &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=2}^T \log \Bigg( \frac{q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0)}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} \cdot \frac{q(\mathbf{z}_t \vert \mathbf{z}_0)}{q(\mathbf{z}_{t-1}\vert\mathbf{z}_0)} \Bigg) + \log \frac{q(\mathbf{z}_1 \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)} \Bigg] \tag{24} \\ &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0)}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{z}_t \vert \mathbf{z}_0)}{q(\mathbf{z}_{t-1} \vert \mathbf{z}_0)} + \log\frac{q(\mathbf{z}_1 \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)} \Bigg] \tag{25} \end{align} $$ Here, since the term $\sum_{t=2}^T \log \frac{q(\mathbf{z}_t \vert \mathbf{z}_0)}{q(\mathbf{z}_{t-1} \vert \mathbf{z}_0)}$ can be unrolled as: $$ \sum_{t=2}^T \log \frac{q(\mathbf{z}_t \vert \mathbf{z}_0)}{q(\mathbf{z}_{t-1} \vert \mathbf{z}_0)} = \log \left( \frac{q(z_2 \vert z_0) q(z_3 \vert z_0) q(z_4 \vert z_0) \cdots q(z_T \vert z_0)}{q(z_1 \vert z_0) q(z_2 \vert z_0) q(z_3 \vert z_0) \cdots q(z_{T-1} \vert z_0)} \right) \tag{26} $$ Where the intermediate terms cancel out, resulting in $\log \frac{q(\mathbf{z}_T \vert \mathbf{z}_0)}{q(\mathbf{z}_{1} \vert \mathbf{z}_0)}$, we can then continue the derivation as follows: $$ \begin{align} L_\text{vub} &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0)}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} + \sum_{t=2}^T \log \frac{q(\mathbf{z}_t \vert \mathbf{z}_0)}{q(\mathbf{z}_{t-1} \vert \mathbf{z}_0)} + \log\frac{q(\mathbf{z}_1 \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)} \Bigg] \tag{27} \\ &amp;= \mathbb{E}_q \Bigg[ -\log p(\mathbf{z}_T) + \sum_{t=2}^T \log \frac{q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0)}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} + \log\frac{q(\mathbf{z}_T \vert \mathbf{z}_0)}{q(\mathbf{z}_1 \vert \mathbf{z}_0)} + \log \frac{q(\mathbf{z}_1 \vert \mathbf{z}_0)}{p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)} \Bigg] \tag{28} \\ &amp;= \mathbb{E}_q \Bigg[ \log\frac{q(\mathbf{z}_T \vert \mathbf{z}_0)}{p(\mathbf{z}_T)} + \sum_{t=2}^T \log \frac{q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0)}{p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t)} - \log p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1) \Bigg], \quad \text{using Logarithmic rules} \tag{29} \\ &amp;= \mathbb{E}_q \Bigg[ \underbrace{D_\text{KL}(q(\mathbf{z}_T \vert \mathbf{z}_0) \parallel p(\mathbf{z}_T))}_{L_T} + \sum_{t=2}^T \underbrace{D_\text{KL}(q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0) \parallel p_\theta(\mathbf{z}_{t-1} \vert\mathbf{z}_t))}_{L_{t-1}} \underbrace{- \log p_\theta(\mathbf{z}_0 \vert \mathbf{z}_1)}_{L_0} \Bigg] \tag{30} \end{align} $$ Recall that in Equation (2), the forward process requires a variance term $\beta_t$, which can either be learned through reparameterization ([Kingma and Welling (2013)](https://arxiv.org/abs/1312.6114)) or treated as hyperparameters fixed as time-dependent constants, both retaining the same functional form ([Sohl-Dickstein et al. (2015)](https://arxiv.org/abs/1503.03585)). The subsequent groundbreaking work by [Ho et al. (2020)](https://arxiv.org/abs/2006.11239) adopts the latter approach. Specifically, $\beta_t$ is set according to a linearly increasing schedule within a specific range based on the timesteps, although subsequent work ([Nichol and Dhariwal (2021)](https://arxiv.org/abs/2102.09672)) has proposed other scheduling schemes. Given that we have fixed the forward process variances $\beta_t$, in Equation (30), since $\mathbf{z}_T$ is Gaussian noise sampled from $\mathcal{N}(0, I)$ and $q$ has no learnable parameters, while $p(\mathbf{z}_T)$ converges to the normal distribution $\mathcal{N}(0, I)$ given a sufficiently large $T$. Therefore, the $L_T$ term is constant and typically small, allowing it to be ignored, and we only need to focus on minimizing the $L_{t-1}$ and $L_0$ terms. Recall that conditioning on $\mathbf{z}_0$ in Equation (24) makes the reverse posterior tractable, let: $$ q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0) = \mathcal{N}(\mathbf{z}_{t-1}; {\tilde{\mu}}(\mathbf{z}_t, \mathbf{z}_0), {\tilde{\beta}_t} \mathbf{I}) \tag{31} $$ Also recall that the Gaussian density function $\mathcal{N}(z; \mu, \sigma^2)$ can be written as: $$ \begin{align} \mathcal{N}(z; \mu, \sigma^2) &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(z - \mu)^2}{2\sigma^2}\right) \tag{32} \\ &amp;\propto \exp\left(-\frac{(z - \mu)^2}{2\sigma^2}\right) \quad = \exp\left(-\frac{(z^2 - 2z\mu + \mu^2)}{2\sigma^2}\right) \tag{33} \end{align} $$ We can then derive the followings: $$ \begin{align} q(\mathbf{z}_{t-1} \vert \mathbf{z}_t, \mathbf{z}_0) &amp;= q(\mathbf{z}_t \vert \mathbf{z}_{t-1}, \mathbf{z}_0) \frac{ q(\mathbf{z}_{t-1} \vert \mathbf{z}_0) }{ q(\mathbf{z}_t \vert \mathbf{z}_0) }, \quad \text{using Bayes' rule} \tag{34} \\ &amp;\propto \exp \left( -\frac{1}{2} \left( \frac{(\mathbf{z}_t - \sqrt{\alpha_t} \mathbf{z}_{t-1})^2}{\beta_t} \right. \right. \notag \\ &amp;\qquad \left. \left. + \frac{(\mathbf{z}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{z}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{z}_t - \sqrt{\bar{\alpha}_t} \mathbf{z}_0)^2}{1-\bar{\alpha}_t} \right) \right) \tag{35} \\ &amp;= \exp \left( -\frac{1}{2} \left( \frac{\mathbf{z}_t^2 - 2\sqrt{\alpha_t} \mathbf{z}_t \mathbf{z}_{t-1} + \alpha_t \mathbf{z}_{t-1}^2}{\beta_t} \right. \right. \notag \\ &amp;\qquad \left. \left. + \frac{\mathbf{z}_{t-1}^2 - 2 \sqrt{\bar{\alpha}_{t-1}} \mathbf{z}_0 \mathbf{z}_{t-1} + \bar{\alpha}_{t-1} \mathbf{z}_0^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{z}_t - \sqrt{\bar{\alpha}_t} \mathbf{z}_0)^2}{1-\bar{\alpha}_t} \right) \right) \tag{36} \\ &amp;= \exp \left( -\frac{1}{2} \left( \left( \frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}} \right) \mathbf{z}_{t-1}^2 \right. \right. \notag \\ &amp;\qquad \left. \left. - \left( \frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{z}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{z}_0 \right) \mathbf{z}_{t-1} + \Lambda(\mathbf{z}_t, \mathbf{z}_0) \right) \right) \tag{37} \end{align} $$ Where the function $\Lambda(\mathbf{z}_t, \mathbf{z}_0)$ is irrelevant of $\mathbf{z}_{t-1}$, and hence is treated as a constant being ignored. By identifying the coefficients in Equation (37) as the form of: $$ -\frac{1}{2} \left( A z_{t-1}^2 - 2 B z_{t-1} + \Lambda \right) \tag{38} $$ With reference to Equation (31), we have: $$ \begin{align} \tilde{\beta}_t &amp;= \frac{1}{A} = (\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}})^{-1} = \frac{\beta_t (1 - \bar{\alpha}_{t-1})}{\alpha_t - \bar{\alpha}_t + \beta_t} = {\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t} \tag{39} \\ \tilde{\mu}_t (\mathbf{z}_t, \mathbf{z}_0) &amp;= \tilde{\beta}_tB = \Big(\frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{z}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{z}_0\Big)\Big(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\Big)^{-1} \tag{40} \\ &amp;= \left( \frac{\sqrt{\alpha_t}}{\beta_t} \mathbf{z}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{z}_0 \right) \Big({\frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t}\Big) \tag{41} \\ &amp;= \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{z}_t + \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} \mathbf{z}_0 \tag{42} \end{align} $$ Recall Equation (10), which allows us to directly sample $\mathbf{z}_t$ from $\mathbf{z}_0$ at any arbitrary $t$. Similarly, we can perform the reverse to obtain $\mathbf{z}_0$ directly from $\mathbf{z}_t$ at any arbitrary $t$, as follows: $$ \mathbf{z}_0 = \frac{\mathbf{z}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}}{\sqrt{\bar{\alpha}_t}} \tag{43} $$ Plugging into $\tilde{\mu}_t (\mathbf{z}_t, \mathbf{z}_0)$ in Equation (42), we can further simplify to: $$ \begin{align} \tilde{\mu}_t &amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{z}_t + \Big(\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t}\Big) \Big(\frac{\mathbf{z}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}}{\sqrt{\bar{\alpha}_t}}\Big) \tag{44} \\ &amp;= {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{z}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \Big)} \tag{45} \end{align} $$ Revisiting the reverse posterior in Equation (4), following the configuration as ([Ho et al. (2020)](https://arxiv.org/abs/2006.11239)), the reverse process variance $\boldsymbol{\Sigma}_\theta(\mathbf{z}_t, t) = \sigma^2_t \mathbf{I}$ is also set to be untrained time-dependent constants. Here, $\sigma^2_t$ is either $\beta_t$ or $\tilde{\beta}_t = \frac{1 - \bar{\alpha}_{t-1}}{1 - \bar{\alpha}_t} \cdot \beta_t$, both of which are claimed to yield similar empirical results. It is noteworthy that subsequent work ([Nichol and Dhariwal (2021)](https://arxiv.org/abs/2102.09672)) has proposed learning the variance $\boldsymbol{\Sigma}_\theta(\mathbf{z}_t, t)$ as an interpolation between $\beta_t$ and $\tilde{\beta}_t$, providing an alternative scheme. Returning to Equation (30), where we aim to minimize the $L_{t-1}$ term, given Equation (4) and Equation (31), recall that the $\beta_t$ term, which their variances depend on, is fixed to constants. This leaves only the mean $\mu_{\theta}(\mathbf{z}_{t}, t)$ to match. We can then parameterize $L_{t-1}$ as follows: $$ L_{t-1} = \mathbb{E}_{\mathbf{z}_0, \epsilon} \Bigg[ \frac{1}{2\sigma_t^2} \left\Vert \tilde{\mu}_t (\mathbf{z}_t, \mathbf{z}_0) - \mu_\theta (\mathbf{z}_t, t) \right\Vert^2 \Bigg] \tag{46} $$ Recall Equation (45), which we want the trained $\mu_{\theta}(\mathbf{z}_{t}, t)$ to match given $\mathbf{z}_t$. Since $\mathbf{z}_t$ is available as input during training, we can instead choose to fit the model on the noise, as derived below: $$ \begin{align} L_{t-1} &amp;= \mathbb{E}_{\mathbf{z}_0, \epsilon} \Bigg[ \frac{1}{2\sigma_t^2} \left\Vert \tilde{\mu}_t (\mathbf{z}_t, \mathbf{z}_0) - \mu_\theta (\mathbf{z}_t, t) \right\Vert^2 \Bigg] \tag{47} \\ &amp;= \mathbb{E}_{\mathbf{z}_0, \epsilon} \Bigg[ \frac{1}{2\sigma_t^2} \left\Vert {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{z}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon} \Big)} - {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{z}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_\theta(\mathbf{z}_t, t) \Big)} \right\Vert^2 \Bigg] \tag{48} \\ &amp;= \mathbb{E}_{\mathbf{z}_0, \epsilon} \Bigg[ \frac{(1 - \alpha_t)^2}{2\sigma_t^2 \alpha_t (1 - \bar{\alpha}_t) } \left\Vert \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta (\mathbf{z}_t, t) \right\Vert^2 \Bigg] \tag{49} \\ &amp;= \mathbb{E}_{\mathbf{z}_0, \epsilon} \Bigg[ \frac{(1 - \alpha_t)^2}{2\sigma_t^2 \alpha_t (1 - \bar{\alpha}_t) } \left\Vert \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta (\mathbf{z}_t, t) \right\Vert^2 \Bigg] \tag{50} \\ &amp;= \mathbb{E}_{\mathbf{z}_0, \epsilon} \Bigg[ \frac{(1 - \alpha_t)^2}{2\sigma_t^2 \alpha_t (1 - \bar{\alpha}_t) } \left\Vert \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta (\sqrt{\bar{\alpha}_t} \mathbf{z}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}, t) \right\Vert^2 \Bigg] \tag{51} \end{align} $$ Here, $\epsilon_\theta$ refers to the model predicting the noise given $z_t$. Furthermore, [Ho et al. (2020)](https://arxiv.org/abs/2006.11239) found that simplifying the objective to the following form, without the coefficient term, works even better during training. This simplified objective is termed $L_{\text{simple}}$: $$ \begin{align} \mathbb{E}_{t, \mathbf{z}_0, \epsilon} \Bigg[ \left\Vert \boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta (\sqrt{\bar{\alpha}_t} \mathbf{z}_0 + \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}, t) \right\Vert^2 \Bigg] := L_{\text{simple}} \tag{52} \end{align} $$ For the $L_0$ term in Equation (30), [Ho et al. (2020)](https://arxiv.org/abs/2006.11239) use a separate discrete decoder, which results in simply avoiding noise addition when $t=1$ during sampling. Consequently, we arrive at $L_{\text{simple}}$ being the sole objective to train diffusion models, termed denoising diffusion probabilistic models (DDPM) ([Ho et al. (2020)](https://arxiv.org/abs/2006.11239)). Algorithm 1: DDPM Training $$ \begin{aligned} &amp;\\ &amp;1. \text{ Repeat:} \\ &amp;2. \quad \mathbf{z}_0 \sim q(\mathbf{z}_0) \\ &amp;3. \quad t \sim \text{Uniform}(\{1, \ldots, T\}) \\ &amp;4. \quad \epsilon \sim \mathcal{N}(0, \mathbf{I}) \\ &amp;5. \quad \text{Take gradient descent step on } \nabla_\theta \left\Vert \epsilon - \epsilon_\theta \left( \sqrt{\bar{\alpha}_t} \mathbf{z}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t \right) \right\Vert^2 \\ &amp;6. \text{ Until converged} \end{aligned} $$ Algorithm 2: DDPM Sampling $$ \begin{aligned} 1. &amp; \quad \mathbf{z}_T \sim \mathcal{N}(0, \mathbf{I}) \\ 2. &amp; \quad \text{For } t = T, \ldots, 1: \\ 3. &amp; \quad \quad \mathbf{z} \sim \mathcal{N}(0, \mathbf{I}) \text{ if } t &gt; 1, \text{ else } \mathbf{z} = 0 \\ 4. &amp; \quad \quad \mathbf{z}_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{z}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta (\mathbf{z}_t, t) \right) + \sigma_t \mathbf{n} \quad n \in \mathcal{N}(0, \mathbf{I}) \\ 5. &amp; \quad \text{Return } \mathbf{z}_0 \end{aligned} $$ Algorithm 1 and Algorithm 2 outline the training and sampling processes of DDPM ([Ho et al. (2020)](https://arxiv.org/abs/2006.11239)), respectively. Note that the sampling scheme follows Equation (4) with the aforementioned settings. However, this sampling scheme is significantly less efficient compared to other generative models such as Generative Adversarial Networks (GANs), as it could potentially involve a singificant number of sampling iterations to obtain a good result. Subsequent work by [Song et al. (2021)](https://arxiv.org/abs/2010.02502), termed as DDIM ([Song et al. (2021)](https://arxiv.org/abs/2010.02502)) addresses this inefficiency by generalizing the forward and reverse diffusion processes to non-Markovian ones. Specifically, with the exact same training objective as in DDPM, another sampling scheme is proposed to sample $\mathbf{z}_{s}$ given $\mathbf{z}_t$, where $s &lt; t$: $$ \mathbf{z}_{s} = \sqrt{\bar{\alpha}_{s}} \left( \frac{\mathbf{z}_t - \sqrt{1 - \bar{\alpha}_t} \epsilon_\theta (\mathbf{z}_t)}{\sqrt{\bar{\alpha}_t}} \right) + \sqrt{1 - \bar{\alpha}_{s} - \sigma_t^2} \cdot \epsilon_\theta (\mathbf{z}_t) + \sigma_t \epsilon \tag{53} $$ Here, $\epsilon \in \mathcal{N}(0, \mathbf{I})$. Note that when $\sigma_t = \sqrt{\frac{(1 - \bar{\alpha}_{t-1})}{(1 - \bar{\alpha}_t)} \cdot \frac{1 - \bar{\alpha}_t}{\bar{\alpha}_{t-1}}} \quad \text{for all } t$ and $s = t - 1$ at all timesteps $t \in [1, T]$, the processes resemble those of DDPM. Conversely, when $\sigma_t = 0$ for all $t$, the forward process (and hence the reverse process) becomes deterministic, leaving the only stochasticity to the initial diffusion latent. Other sampling schedules have been proposed ([Karras et al. (2022)](https://arxiv.org/abs/2206.00364), [Liu et al. (2022)](https://arxiv.org/abs/2202.09778), [Song et al. (2023)](https://arxiv.org/abs/2303.01469), [Zhang et al. (2023)](https://arxiv.org/abs/2303.09556)) to facilitate fast generation while maintaining high quality. <h4 style="display:block !important; visibility:visible !important; font-size:1.25rem !important; margin-top:2rem !important; margin-bottom:1rem !important;">Diffusion Guidance</h4> Guidance methods have been proposed to control the strength of steering the diffusion generation towards catering the faithfulness of certain classes or conditions, specifically, the classifier guidance ([Dhariwal and Nichol (2021)](https://arxiv.org/abs/2105.05233)) and classifier-free guidance ([Ho and Salimans (2022)](https://arxiv.org/abs/2207.12598)). Classifier guidance aims at explicitly incorporating class information during sampling via the gradients of a noise-aware classifier $f_\phi$. Given the class information $y$, we have: $$ \begin{align} \nabla_{\mathbf{z}_t} \log q(\mathbf{z}_t \vert y) &amp;= \nabla_{\mathbf{z}_t} \log \left( \frac{q(\mathbf{z}_t) q(y \vert \mathbf{z}_t)}{q(y)} \right), \quad \text{using Bayes' rule} \tag{54} \\ &amp;= \nabla_{\mathbf{z}_t} \log q(\mathbf{z}_t) + \nabla_{\mathbf{z}_t} \log q(y \vert \mathbf{z}_t) - \nabla \log q(y) \tag{55} \\ &amp;= \nabla_{\mathbf{z}_t} \log q(\mathbf{z}_t) + \nabla_{\mathbf{z}_t} \log q(y \vert \mathbf{z}_t) \tag{56} \\ &amp; \approx \nabla_{\mathbf{z}_t} \log p_\theta(\mathbf{z}_t) + \nabla_{\mathbf{z}_t} \log f_\phi(y \vert \mathbf{z}_t) \tag{57} \end{align} $$ Where $\nabla \log q(y)$ is irrelevant to $z_t$ and thus can be ignored. In ([Dhariwal and Nichol (2021)](https://arxiv.org/abs/2105.05233)), a score-based conditioning trick from ([Song and Ermon (2021)](https://arxiv.org/abs/2011.13456)) that draws a connection between diffusion models and score matching ([Song and Ermon (2020)](https://arxiv.org/abs/1907.05600)) is used, giving: $$ \nabla_{\mathbf{z}_t} \log p_\theta (\mathbf{z}_t) = - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta (\mathbf{z}_t) \tag{58} $$ Where $\epsilon_\theta$ is the noise-predictor model. Then, by substituting into the score function, we have: $$ \begin{align} \nabla_{\mathbf{z}_t} \log q(\mathbf{z}_t \vert y) &amp; \approx \nabla_{\mathbf{z}_t} \log p_\theta(\mathbf{z}_t) + \nabla_{\mathbf{z}_t} \log f_\phi(y \vert \mathbf{z}_t) \tag{59} \\ &amp;= - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} (\boldsymbol{\epsilon}_\theta(\mathbf{z}_t, t) - \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{z}_t} \log f_\phi(y \vert \mathbf{z}_t)) \tag{60} \\ &amp;= - \frac{1}{\sqrt{1 - \bar{\alpha}_t}} (\boldsymbol{\epsilon}_\theta(\mathbf{z}_t, t) - \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{z}_t} \log f_\phi(y \vert \mathbf{z}_t)) \tag{61} \end{align} $$ Which leads to a new noise prediction that integrates the score from the classifier: $$ \hat{\boldsymbol{\epsilon}}_\theta(\mathbf{z}_t, t) = \boldsymbol{\epsilon}_\theta(x_t, t) - s \cdot \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{z}_t} \log f_\phi(y \vert \mathbf{z}_t) \tag{62} $$ Where $s$ denotes the constant guidance scale, which modulates the trade-off between sample fidelity and diversity. A larger scale enhances the fidelity and faithfulness to the class but reduces the diversity of the generated samples. On the other hand, classifier-free guidance ([Ho and Salimans (2022)](https://arxiv.org/abs/2207.12598)) adheres to a similar intuition but functions without an explicit classifier. During training, classifier-free guidance employs a scheme that randomly masks the conditioning information, thereby capturing both the conditional and unconditional distributions. This method enables extrapolation with a specified guidance scale during sampling to achieve comparable trade-offs. For more details, we refer readers to the original paper ([Ho and Salimans (2022)](https://arxiv.org/abs/2207.12598)) for a comprehensive discussion. <h4 style="display:block !important; visibility:visible !important; font-size:1.25rem !important; margin-top:2rem !important; margin-bottom:1rem !important;">Latent Diffusion Models</h4> Another notable advancement in diffusion models is the advent of Latent Diffusion Models ([Rombach et al. (2022)](https://arxiv.org/abs/2112.10752)). The core concept involves performing the diffusion process on smaller spatial latent representations of the original images, defined by pre-trained Variational Autoencoders (VAE). This approach significantly reduces the computational overhead associated with training on high-resolution pixel-level space, while the latent representations produced by VAE are potentially more semantically meaningful. Specifically, using a learned VAE encoder $\mathcal{E}$ and image data $\mathbf{x}$, we train and sample diffusion models on $\mathbf{z} = \mathcal{E}(\mathbf{x})$ while keeping $\mathcal{E}$ fixed. Finally, we apply a VAE decoder $\mathcal{D}$ on $\mathbf{z}$ to obtain the generated samples $\hat{\mathbf{x}} = \mathcal{D}(\mathbf{z})$. </div> <p><br/></p> <h2 id="references">References</h2> <p>[1] L. Weng, “What are diffusion models?,” lilianweng.github.io, Jul. 2021. [Online]. Available: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</p> <p>[2] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep unsupervised learning using nonequilibrium thermodynamics,” in Proceedings of the 32nd International Conference on Machine Learning, vol. 37, pp. 2256–2265, PMLR, 2015.</p> <p>[3] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,” in Advances in Neural Information Processing Systems, vol. 33, pp. 6840–6851, Curran Associates, Inc., 2020.</p> <p>[4] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013.</p> <p>[5] A. Q. Nichol and P. Dhariwal, “Improved denoising diffusion probabilistic models,” in International Conference on Machine Learning, pp. 8162–8171, 2021.</p> <p>[6] J. Song, C. Meng, and S. Ermon, “Denoising diffusion implicit models,” in International Conference on Learning Representations, 2021.</p> <p>[7] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, “High-resolution image synthesis with latent diffusion models,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10684–10695, June 2022.</p> <hr/> <h5 id="thanks-for-reading-if-you-find-any-mistakes-or-have-any-questions-please-feel-free-to-contact-me-d">Thanks for reading! If you find any mistakes or have any questions, please feel free to contact me :D</h5>]]></content><author><name></name></author><category term="machine-learning"/><category term="machine-learning"/><category term="diffusion-models"/><category term="deep-learning"/><category term="generative-models"/><summary type="html"><![CDATA[A comprehensive mathematical explanation of diffusion models, their forward and reverse processes]]></summary></entry><entry><title type="html">Potential Solutions when encountering the nan gradient problem with pytorch</title><link href="https://zyinghua.github.io/blog/2024/grad-nan-solutions/" rel="alternate" type="text/html" title="Potential Solutions when encountering the nan gradient problem with pytorch"/><published>2024-03-26T00:00:00+00:00</published><updated>2024-03-26T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2024/grad-nan-solutions</id><content type="html" xml:base="https://zyinghua.github.io/blog/2024/grad-nan-solutions/"><![CDATA[<h4 id="when-working-with-pytorch-nan-gradient-problem-can-be-common-here-are-the-potential-solutions-that-might-work">When working with pytorch, NaN gradient problem can be common, here are the potential solutions that might work:</h4> <ol> <li> <p>Firstly make sure the <u>inputs</u> do not contain or <u>loss</u> is not <code class="language-plaintext highlighter-rouge">inf</code> or <code class="language-plaintext highlighter-rouge">NaN</code> (e.g., via printing).</p> </li> <li> <p>Make sure there’s no division-by-zero throughout the entire computational graph. Especially also check operations like <code class="language-plaintext highlighter-rouge">x.sqrt()</code> or <code class="language-plaintext highlighter-rouge">x.pow()</code>, make sure the values involved don’t cause mathmatical errors that can happen when they are too small, add an epsilon (e.g., 1e-8) if that’s the case.</p> </li> <li> <p>Sometimes the problem can be caused by low precision rate: for example, if your tensors involved in the computation are torch.float16, try change to float32 by <code class="language-plaintext highlighter-rouge">tensor.to(torch.float32)</code>, that can help in reducing numerical instability, potentially resolving the issue, though at the cost of increased computational resources.</p> </li> <li> <p>Make sure the learning rate is not too large if it is involved in the problem. Also try gradient clipping to prevent gradients from becoming too large.</p> </li> <li> <p>Sometimes calling <code class="language-plaintext highlighter-rouge">torch.autograd.set_detect_anomaly(True)</code> as a starting point may help.</p> </li> </ol>]]></content><author><name></name></author><category term="pytorch"/><category term="pytorch"/><category term="deep-learning"/><category term="nan-gradient"/><category term="troubleshooting"/><summary type="html"><![CDATA[A guide on troubleshooting and resolving NaN gradient issues in PyTorch deep learning models]]></summary></entry><entry><title type="html">Conda Setup Diary</title><link href="https://zyinghua.github.io/blog/2023/conda-setup-diary/" rel="alternate" type="text/html" title="Conda Setup Diary"/><published>2023-09-13T00:00:00+00:00</published><updated>2023-09-13T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2023/conda-setup-diary</id><content type="html" xml:base="https://zyinghua.github.io/blog/2023/conda-setup-diary/"><![CDATA[<h4 id="this-is-the-general-steps-on-top-of-my-head-that-i-followed-to-set-up-the-conda-environment-on-a-computing-server-for-a-new-user-account-which-might-be-commonly-encountered-when-doing-ai-research">This is the general steps on top of my head, that I followed to set up the conda environment on a computing server for a new user account, which might be commonly encountered when doing AI research.</h4> <hr/> <ol> <li>Log onto the server via SSH: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> ssh &lt;username&gt;@&lt;server-ip&gt; <span class="nt">-p</span> &lt;port&gt;
</code></pre></div> </div> <p>Make sure you activate the VPN if necessary. You will then be prompted to enter the password for the user account.</p> </li> <li>Once logged in, you will need to set up for environments, in AI research, typically we use conda. Make sure to download the latest version of miniconda from the official website, and install it on the server. At this time, the command to download the latest miniconda is: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
</code></pre></div> </div> <p>Once downloaded, run the script as:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">chmod</span> +x Miniconda3-latest-Linux-x86_64.sh
 ./Miniconda3-latest-Linux-x86_64.sh
</code></pre></div> </div> <p>Once you run the installer, you will be presented with a series of prompts. It’s generally safe to accept the defaults. When asked if you’d like to prepend the Miniconda3 install location to PATH in your ~/.bashrc, you can opt to say ‘yes’. This ensures that you can activate Conda from any new terminal.</p> </li> <li>Close and reopen your terminal or use the command to reload the profile: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda init bash
</code></pre></div> </div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">source</span> ~/.bashrc
</code></pre></div> </div> </li> <li>Once conda is installed, you can create a new environment for your project, by running: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda create <span class="nt">-n</span> &lt;env-name&gt; <span class="nv">python</span><span class="o">=</span>&lt;python-version&gt; <span class="nv">tensorflow</span><span class="o">=</span>&lt;tensorflow-version&gt;
</code></pre></div> </div> <p>The python and tensorflow version specification are optional, when specified, the environment will be created with the specified python and tensorflow version.</p> <p>Or if you have a environment.yaml file in the repo, you can do:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda <span class="nb">env </span>create <span class="nt">-f</span> environment.yaml
</code></pre></div> </div> <p>which takes care of the installation of all necessary packages.</p> </li> <li>you can then activate the environment by running: <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>conda activate &lt;env-name&gt;
</code></pre></div> </div> <p>And deactivate it by running:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> conda deactivate
</code></pre></div> </div> </li> </ol> <h4 id="note-you-may-need-to-run-source-bashrc-at-the-time-you-enter-the-server-to-make-sure-the-conda-command-is-available"><strong>Note:</strong> You may need to run <code class="language-plaintext highlighter-rouge">source ~/.bashrc</code> at the time you enter the server, to make sure the conda command is available.</h4> <hr/> <h3 id="some-useful-commands">Some useful commands:</h3> <ol> <li> <p>Install package via conda:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">~$ conda install &lt;package-name&gt;=&lt;package-version:optional&gt;</code></p> </blockquote> <p>To uninstall:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">~$ conda uninstall &lt;package-name&gt;</code></p> </blockquote> </li> <li> <p>Install package via pip:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">~$ pip install &lt;package-name&gt;=&lt;package-version:optional&gt;</code></p> </blockquote> </li> <li> <p>Check a specific package installed:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">~$ conda list &lt;package-name&gt;</code></p> </blockquote> </li> <li> <p>Check all conda env created:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">~$ conda env list</code></p> </blockquote> </li> <li> <p>Remove a conda env:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">~$ conda env remove -n &lt;env-name&gt;</code></p> </blockquote> </li> <li> <p>Check current GPU status on the server:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">~$ nvidia-smi</code></p> </blockquote> <p>OR check live gpu status:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">gpustat -i</code></p> </blockquote> </li> <li> <p>Open up a sub-window system:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">tmux</code></p> </blockquote> <p>within tmux:</p> <ul> <li> <p>Create a new window:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ctrl + b</code> then <code class="language-plaintext highlighter-rouge">c</code></p> </blockquote> </li> <li> <p>Switch to the next window:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ctrl + b</code> then <code class="language-plaintext highlighter-rouge">n</code></p> </blockquote> </li> <li> <p>Switch to the previous window:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ctrl + b</code> then <code class="language-plaintext highlighter-rouge">p</code></p> </blockquote> </li> <li> <p>Detach from the current window:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Ctrl + b</code> then <code class="language-plaintext highlighter-rouge">d</code></p> </blockquote> </li> <li> <p>Delete the current window:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">exit</code></p> </blockquote> </li> </ul> <p>Outside tmux:</p> <ul> <li> <p>Check all the windows:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">tmux ls</code></p> </blockquote> </li> <li> <p>Attach to a specific window:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">tmux attach -t &lt;window-name&gt;</code></p> </blockquote> </li> </ul> </li> <li> <p>Run a python script using specific GPU:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">CUDA_VISIBLE_DEVICES=&lt;gpu-id1&gt;,&lt;gpu-id2&gt;,... python &lt;script-name&gt;.py</code></p> </blockquote> </li> <li> <p>If you ever need to kill a process, you can use the command:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">kill -9 &lt;process-id&gt;</code></p> </blockquote> <p>Process ID can be found by calling <code class="language-plaintext highlighter-rouge">nvidia-smi</code>.</p> </li> <li> <p>To upload a file from local to the server:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">scp -P &lt;port&gt; &lt;local-file-path&gt; &lt;username&gt;@&lt;server-ip&gt;:&lt;server-file-path&gt;</code></p> </blockquote> <p>e.g.:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">scp -P 22 /Users/username/Desktop/test.txt username@server-ip:~/home/username/test.txt</code></p> </blockquote> </li> </ol>]]></content><author><name></name></author><category term="tutorials"/><category term="conda"/><category term="computing-server"/><category term="research-experiments"/><summary type="html"><![CDATA[A guide on setting up conda environments on a computing server for AI research]]></summary></entry><entry><title type="html">Leetcode 904: Fruit Into Baskets</title><link href="https://zyinghua.github.io/blog/2023/leetcode-904/" rel="alternate" type="text/html" title="Leetcode 904: Fruit Into Baskets"/><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2023/leetcode-904</id><content type="html" xml:base="https://zyinghua.github.io/blog/2023/leetcode-904/"><![CDATA[<h3 id="intuition">Intuition</h3> <p>Question = Find the longest sub-string consists of only two numbers.</p> <h3 id="approach">Approach</h3> <p>Keep track of the last seen number before the current index, while Having a pointer to the <code class="language-plaintext highlighter-rouge">start</code> of such a valid sub-string that ends up in the current index (inclusively); Or if the number of the current index cannot be included in the current sub-string (The first third number, which terminates the construction), update the maximum, set <code class="language-plaintext highlighter-rouge">start</code> to the starting index of the last seen number (which refers to the longest valid sub-string, should the current number be included). And then update the basket, which is used to keep track of the two numbers (fruits) of the current sub-string.</p> <p>If <code class="language-plaintext highlighter-rouge">start</code> cannot make the sub-string ending up at <code class="language-plaintext highlighter-rouge">i</code> (inclusively) valid, then any starting index before <code class="language-plaintext highlighter-rouge">start</code> cannot. If <code class="language-plaintext highlighter-rouge">start</code> can make the sub-string ending up at <code class="language-plaintext highlighter-rouge">i</code> (inclusively) valid, then any starting index between <code class="language-plaintext highlighter-rouge">start</code> and <code class="language-plaintext highlighter-rouge">i</code> can.</p> <p>Complexity:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Time complexity: O(n)
Space complexity: O(1)
</code></pre></div></div> <h3 id="code">Code</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">totalFruit</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">fruits</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">max_fruits</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">last_candidate</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">fruits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">basket</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">fruits</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">fruits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">basket</span><span class="p">:</span>
                <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">basket</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
                    <span class="n">basket</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">fruits</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">max_fruits</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">max_fruits</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
                    <span class="n">start</span> <span class="o">=</span> <span class="n">last_candidate</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">basket</span> <span class="o">=</span> <span class="p">[</span><span class="n">fruits</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">last_candidate</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
            
            <span class="k">if</span> <span class="n">fruits</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">!=</span> <span class="n">last_candidate</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
                <span class="n">last_candidate</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">fruits</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">return</span> <span class="nf">max</span><span class="p">(</span><span class="n">max_fruits</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">fruits</span><span class="p">)</span> <span class="o">-</span> <span class="n">start</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="leetcode"/><category term="leetcode"/><category term="algorithms"/><category term="solution"/><summary type="html"><![CDATA[A solution to the Leetcode problem of finding the longest sub-string consisting of only two numbers]]></summary></entry><entry><title type="html">Leetcode 1359: Count All Valid Pickup and Delivery Options</title><link href="https://zyinghua.github.io/blog/2022/leetcode-1359/" rel="alternate" type="text/html" title="Leetcode 1359: Count All Valid Pickup and Delivery Options"/><published>2022-03-06T00:00:00+00:00</published><updated>2022-03-06T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2022/leetcode-1359</id><content type="html" xml:base="https://zyinghua.github.io/blog/2022/leetcode-1359/"><![CDATA[<h3 id="general-idea">General Idea</h3> <p>Basically the idea is to iteratively count the specific “number of orders” based on the number of possible sequences for “number of orders - 1”; and the number of possibilities of placing Pickup and Delivery in each previous sequence, which can be calculated by summing up the first <code class="language-plaintext highlighter-rouge">((number of orders - 1) * 2 + 1)</code> natural numbers.</p> <p>The formula might be a bit vague at the first glance, let’s have a look at an example first:</p> <p>Suppose we are dealing with 2 orders, and we have already known that for 1 order the answer is 1, which the only possible sequence is <code class="language-plaintext highlighter-rouge">(P1, D1)</code>. We want to insert <code class="language-plaintext highlighter-rouge">P2</code> and <code class="language-plaintext highlighter-rouge">D2</code> somewhere such that <code class="language-plaintext highlighter-rouge">D2</code> is always after <code class="language-plaintext highlighter-rouge">P2</code>, so let’s think about where we can insert <code class="language-plaintext highlighter-rouge">P2</code> first.</p> <p>We can insert P2 at any position in <code class="language-plaintext highlighter-rouge">(P1, D1)</code>, and all the possibilities are: <code class="language-plaintext highlighter-rouge">(P2, P1, D1)</code>, <code class="language-plaintext highlighter-rouge">(P1, P2, D1)</code> and <code class="language-plaintext highlighter-rouge">(P1, D1, P2)</code> with the indices being <code class="language-plaintext highlighter-rouge">[0, 1, 2]</code>, which the number of possibilities of placing P2 is exactly <code class="language-plaintext highlighter-rouge">k * 2 + 1</code>, where <code class="language-plaintext highlighter-rouge">k</code> represents the number of orders - 1, and <code class="language-plaintext highlighter-rouge">* 2</code> means each order has 2 states (Pickup and Delivery).</p> <p>Why? You can think of it as we can insert <code class="language-plaintext highlighter-rouge">P2</code> right after any index, plus <code class="language-plaintext highlighter-rouge">P2</code> being the first before all the others, that doesn’t violate the conditions since we haven’t inserted/considered <code class="language-plaintext highlighter-rouge">D2</code> yet.</p> <p>Now we can imagine where to put <code class="language-plaintext highlighter-rouge">D2</code> when <code class="language-plaintext highlighter-rouge">P2</code> is settled down, it can be anywhere as long as after <code class="language-plaintext highlighter-rouge">P2</code>, right? So when <code class="language-plaintext highlighter-rouge">P2</code> is at index 0, there can be 3 possibilities <code class="language-plaintext highlighter-rouge">D2</code> can be placed, right after <code class="language-plaintext highlighter-rouge">P2</code> at index <code class="language-plaintext highlighter-rouge">1 - [P2, D2, P1, D1]</code>, or after <code class="language-plaintext highlighter-rouge">P1 - [P2, P1, D2, D1]</code>, or after <code class="language-plaintext highlighter-rouge">D1 - [P2, P1, D1, D2]</code>. And when <code class="language-plaintext highlighter-rouge">P2</code> is at index 1, there can be 2 possibilities (<code class="language-plaintext highlighter-rouge">D2</code> right after <code class="language-plaintext highlighter-rouge">P2 - [P1, P2, D2, D1]</code>, or at <code class="language-plaintext highlighter-rouge">the last index - [P1, P2, D1, D2]</code>). And lastly when <code class="language-plaintext highlighter-rouge">P2</code> is at index 2, the only possibility is <code class="language-plaintext highlighter-rouge">[P1, D1, P2, D2]</code>. Therefore, <code class="language-plaintext highlighter-rouge">1 * (3 + 2 + 1) = 6</code> in total, being the answer for 2 orders.</p> <p>From the above example, we can observe the pattern: When Pickup is at index say <code class="language-plaintext highlighter-rouge">x</code>, The number of possibilities Delivery of the same number of orders can be placed is of <code class="language-plaintext highlighter-rouge">(k * 2 + 1) - x</code>, which means anywhere after <code class="language-plaintext highlighter-rouge">x</code> (inclusively, meaning right after Pickup). Since <code class="language-plaintext highlighter-rouge">x</code> can be anything in <code class="language-plaintext highlighter-rouge">[0 .. k * 2]</code>, therefore the number of possibilities to place Pickup and Delivery, given a sequence of <code class="language-plaintext highlighter-rouge">k</code> (i.e., <code class="language-plaintext highlighter-rouge">number of orders - 1</code>), is calculated by <code class="language-plaintext highlighter-rouge">((k * 2 + 1) - 0, (k * 2 + 1) - 1, ... (k * 2 + 1) - k * 2)</code>. That is, for example for 3 orders, which 3 - 1 orders that has 4 states - (P1, D1, P2 and D2), the number of possibilities based on a given previous sequence is 5 + 4 + 3 + 2 + 1 = 15, that is the sum of the first <code class="language-plaintext highlighter-rouge">(k * 2 + 1)</code> natural numbers! And since there are 6 possible sequences for the previous number of orders (= 2), therefore for 3 orders, the answer is 6 * 15 = 90.</p> <p>Overall, the answer for n orders, equals to:</p> <blockquote> <p>(The number of possible sequences for n - 1) * (The sum of the first n natural numbers)</p> </blockquote> <h3 id="code">Code</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">countOrders</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">
        The sum of the first n natural numbers: (n + 1) * n / 2.
        e.g. For 10: (1 + 10) + (2 + 9) + (3 + 8) + ..., in total
        that has n / 2 sets such that each set equals to n + 1.
        </span><span class="sh">"""</span>
        
        <span class="n">ans</span> <span class="o">=</span> <span class="mi">1</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="sh">"""</span><span class="s">
            Here i - 1 represents the previous number of orders, (* 2) represents each has 2 states Pickup and Delivery,
            that together gives the total number of elements in each of it</span><span class="sh">'</span><span class="s">s possible sequences. Then + 1
            represents other than placing the Pickup at the end of each element, it can be placed at the first index 
            before everyone else. Together as ((i - 1) * 2 + 1) represents the number of possible places to put the Pickup
            and hence the natural number we need.
            </span><span class="sh">"""</span>
            <span class="n">ans</span> <span class="o">=</span> <span class="n">math</span><span class="p">.</span><span class="nf">floor</span><span class="p">((</span><span class="n">ans</span> <span class="o">*</span> <span class="p">((((</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))</span> <span class="o">%</span> <span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="mi">9</span> <span class="o">+</span> <span class="mi">7</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">ans</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="leetcode"/><category term="leetcode"/><category term="algorithms"/><category term="solution"/><category term="combinatorics"/><summary type="html"><![CDATA[A solution to the Leetcode problem of counting valid pickup and delivery options using combinatorial mathematics]]></summary></entry><entry><title type="html">Commonly Used Git Commands</title><link href="https://zyinghua.github.io/blog/2022/git-commands/" rel="alternate" type="text/html" title="Commonly Used Git Commands"/><published>2022-02-28T00:00:00+00:00</published><updated>2022-02-28T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2022/git-commands</id><content type="html" xml:base="https://zyinghua.github.io/blog/2022/git-commands/"><![CDATA[<h3 id="login--pull">Login &amp; Pull</h3> <p>Set your git user email for every repository on your computer</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git config --global user.email "xxx@email.com"</code></p> </blockquote> <p>Set your git username for every repository on your computer</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git config --global user.name "zyinghua"</code></p> </blockquote> <p>Clone from github/gitlab, etc…:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git clone https://github.com/zyinghua/...</code></p> </blockquote> <p>Pull from the repository cloned while staying in that clone (which contains .git):</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git pull</code></p> </blockquote> <hr/> <h3 id="commit--push">Commit &amp; Push</h3> <ol> <li> <p>Check Status (between the platform and the local repo):</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git status</code></p> </blockquote> </li> <li> <p>Add a newly affected file:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git add newFile.txt</code></p> </blockquote> </li> <li> <p>Add ALL newly affected files:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git add .</code></p> </blockquote> </li> <li> <p>Commit the changes:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git commit -m "Comment message goes here..."</code></p> </blockquote> </li> <li> <p>Push the changes:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git push</code></p> </blockquote> </li> </ol> <hr/> <h3 id="branch--merge">Branch &amp; Merge</h3> <p>To check all the branches:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git branch</code></p> </blockquote> <p>To add a new branch:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git branch branchname</code></p> </blockquote> <p>To change to a specific branch:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git checkout branchname</code></p> </blockquote> <p>Once commited in a non-main branch, the changes will not be visible to the other branches! Until the merge.</p> <p>To create a new empty file:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ touch filename.html (or txt, etc...)</code></p> </blockquote> <p>To edit the file in git bash:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ nano filename.html</code></p> </blockquote> <p>To show differences between the current branch and the other one:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git diff otherbranchname (e.g. main)</code></p> </blockquote> <p>To merge branches:</p> <blockquote> <p>Switch to the main branch ($ git checkout main), and then: <code class="language-plaintext highlighter-rouge">$ git merge branchname</code></p> </blockquote> <h3 id="revert">Revert</h3> <p>Sometimes you would want to revert to a previous commit. To do so, first check the commit history and obtain the commit ID (e.g., a6b1234), then:</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">$ git reset --hard &lt;commit_id&gt;</code></p> <p><code class="language-plaintext highlighter-rouge">$ git push --force</code></p> </blockquote>]]></content><author><name></name></author><category term="git"/><category term="tools"/><category term="git"/><category term="version-control"/><category term="development"/><category term="reference"/><summary type="html"><![CDATA[A reference guide for frequently used Git commands for everyday development workflows]]></summary></entry><entry><title type="html">Leetcode 33: Search in Rotated Sorted Array</title><link href="https://zyinghua.github.io/blog/2022/leetcode-33/" rel="alternate" type="text/html" title="Leetcode 33: Search in Rotated Sorted Array"/><published>2022-01-25T00:00:00+00:00</published><updated>2022-01-25T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2022/leetcode-33</id><content type="html" xml:base="https://zyinghua.github.io/blog/2022/leetcode-33/"><![CDATA[<h3 id="general-idea">General Idea</h3> <p>Basically on top of binary search, we make use of the first item in the list, which gives information about whether the smallest is on the left or right side of the current chosen [mid], and handle each case by case. To clarify: If no rotate is made, the algorithm just works as usual.</p> <p>Explanation at each step within the code below.</p> <h3 id="code">Code</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">nums</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">target</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">left</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">right</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        
        <span class="k">while</span> <span class="n">left</span> <span class="o">&lt;=</span> <span class="n">right</span><span class="p">:</span>
            <span class="n">mid</span> <span class="o">=</span> <span class="p">(</span><span class="n">left</span> <span class="o">+</span> <span class="n">right</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            
            <span class="k">if</span> <span class="n">target</span> <span class="o">&lt;</span> <span class="n">nums</span><span class="p">[</span><span class="n">mid</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">nums</span><span class="p">[</span><span class="n">mid</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">nums</span><span class="p">[</span><span class="n">left</span><span class="p">]:</span>
                    <span class="sh">"""</span><span class="s">
                    Smallest on the left side inclusively,
                    so if target is less than, it is only
                    possible on the left.
                    </span><span class="sh">"""</span>
                    
                    <span class="n">right</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="sh">"""</span><span class="s">
                    If the smallest is on the right side of mid
                    caused by a possible rotate.
                    </span><span class="sh">"""</span>
                    <span class="k">if</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="n">nums</span><span class="p">[</span><span class="n">left</span><span class="p">]:</span>
                        <span class="sh">"""</span><span class="s">
                        nums[left] &lt; target &lt; nums[mid], 
                        since smallest on the right side, 
                        it</span><span class="sh">'</span><span class="s">s a perfect interval (non-decreasing).
                        </span><span class="sh">"""</span>
                        
                        <span class="n">right</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="k">elif</span> <span class="n">target</span> <span class="o">==</span> <span class="n">nums</span><span class="p">[</span><span class="n">left</span><span class="p">]:</span>
                        <span class="k">return</span> <span class="n">left</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="sh">"""</span><span class="s">
                        Target possibly on the second half of 
                        the right part.
                        </span><span class="sh">"""</span>
                        
                        <span class="n">left</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="n">nums</span><span class="p">[</span><span class="n">mid</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">nums</span><span class="p">[</span><span class="n">mid</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">nums</span><span class="p">[</span><span class="n">left</span><span class="p">]:</span>
                    <span class="sh">"""</span><span class="s">Smallest on the left inclusively.</span><span class="sh">"""</span>
                    <span class="k">if</span> <span class="n">target</span> <span class="o">&gt;</span> <span class="n">nums</span><span class="p">[</span><span class="n">right</span><span class="p">]:</span>
                        <span class="sh">"""</span><span class="s">
                        Since smallest on the left, leaving
                        the right part non decreasing. So if 
                        greater, must only possible on the
                        first part of the left.
                        </span><span class="sh">"""</span>
                        <span class="n">right</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="k">elif</span> <span class="n">target</span> <span class="o">==</span> <span class="n">nums</span><span class="p">[</span><span class="n">right</span><span class="p">]:</span>
                        <span class="k">return</span> <span class="n">right</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="sh">"""</span><span class="s">
                        nums[mid] &lt; target &lt; nums[right],
                        since smallest on the left side,
                        it</span><span class="sh">'</span><span class="s">s a perfect interval (non-decreasing).
                        </span><span class="sh">"""</span>
                        <span class="n">left</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="sh">"""</span><span class="s">
                    Smallest on the right = left part non-decreasing,
                    therefore if target greater than mid, must be in 
                    the first part of right if exists.
                    </span><span class="sh">"""</span>
                    <span class="n">left</span> <span class="o">=</span> <span class="n">mid</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># target == nums[mid]:
</span>                <span class="k">return</span> <span class="n">mid</span>
        
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="leetcode"/><category term="leetcode"/><category term="algorithms"/><category term="solution"/><category term="binary-search"/><summary type="html"><![CDATA[A solution to the Leetcode problem of searching in a rotated sorted array with O(log n) complexity]]></summary></entry><entry><title type="html">Leetcode 142: Linked List Cycle II</title><link href="https://zyinghua.github.io/blog/2022/leetcode-142/" rel="alternate" type="text/html" title="Leetcode 142: Linked List Cycle II"/><published>2022-01-20T00:00:00+00:00</published><updated>2022-01-20T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2022/leetcode-142</id><content type="html" xml:base="https://zyinghua.github.io/blog/2022/leetcode-142/"><![CDATA[<p>This question is asking for the node where the cycle begins if there’s any, without modifying the list. It may comes less intuitive at the first glance, therefore we need some kind of algorithms that can detect the cycle in a different way.</p> <h2 id="floyds-cycle-finding-algorithm"><a href="https://en.wikipedia.org/wiki/Cycle_detection">Floyd’s cycle-finding algorithm</a></h2> <p>The idea of this algorithm came up by <a href="https://en.wikipedia.org/wiki/Robert_W._Floyd">Robert W. Floyd</a>, is by using two pointers (tortoise and hare), such that one moves twice faster (2x speed) than the other, if there exists a cycle, the pointers will eventually meet at a certain point &gt;= where the cycle starts.</p> <h5 id="following-the-algorithm-consider-a-single-linked-list-with-a-cycle-like-this">Following the algorithm, consider a single linked list with a cycle like this:</h5> <p><strong>*(Suggest drawing it out to make this clear :D)</strong></p> <blockquote> <p>Head _ _ _ {x}_ _ _ Cycle(S)_ _ _ {y}_ _ _ Meeting point(M)_ _ _ {z}_ _ _ Cycle(S)_ _ _ _</p> </blockquote> <p>We know that at the meeting point (M), the distance travelled by the tortoise (the slow pointer) will be x + y, and for the hare (the fast pointer) will be x + y + z + y = x + 2y + z. Since we know the time is constant and the hare moves 2x speed than the tortoise, therefore we can deduce:</p> <blockquote> <p>2 * (x + y) = x + 2y + z</p> <p>2x + 2y = x + 2y + z</p> <p>2x = x + z</p> <p>x = z</p> </blockquote> <p>Which means Distance (Head to Cycle start) equals to Distance (Meeting point to Cycle start).</p> <h4 id="another-way-to-prove-the-same-statement">Another way to prove the same statement:</h4> <blockquote> <p>Head _ _ _ {x} _ _ _ Cycle(S) _ _ _ {?} _ _ _ Meeting point(M) _ _ _ {?} _ _ _ (2x) _ _ _ {y} _ _ _ Cycle(S) _ _ _</p> </blockquote> <p>Assuming the distance between the Head and Cycle(S) is <code class="language-plaintext highlighter-rouge">x</code>, when the tortoise arrives at Cycle(S), the hare will be at <code class="language-plaintext highlighter-rouge">2x</code>. Assuming the distance between 2x and Cycle(S) by going forward is <code class="language-plaintext highlighter-rouge">y</code>, when the hare reaches Cycle(S) again by following the cycle, the tortoise will be at <code class="language-plaintext highlighter-rouge">x + y/2</code>.</p> <p>So the problem now becomes:</p> <blockquote> <p>The hare moves 2x faster than the tortoise, the distance between them now is a certain value say <code class="language-plaintext highlighter-rouge">y/2</code>, when will the hare catch the tortoise?</p> </blockquote> <p>And the answer is when the hare moves <code class="language-plaintext highlighter-rouge">y</code> distance, after a time of <code class="language-plaintext highlighter-rouge">y/2</code>.</p> <p>If we think about where the hare starts, it’s actually Cycle(S), and after <code class="language-plaintext highlighter-rouge">y</code> distance, they will meet, so the meeting point is at <code class="language-plaintext highlighter-rouge">x + y</code>.</p> <p>We know that 2x to Cycle(S) is <code class="language-plaintext highlighter-rouge">y</code>, and Distance (Meeting point to 2x) is <code class="language-plaintext highlighter-rouge">2x - (x + y)</code> = <code class="language-plaintext highlighter-rouge">x - y</code>, and so Distance (Meeting point to Cycle(S)) is <code class="language-plaintext highlighter-rouge">x - y + y</code> which is <code class="language-plaintext highlighter-rouge">x</code> itself.</p> <p>The other case happens when the meeting point occurs before 2x:</p> <blockquote> <p>Head _ _ _ {x} _ _ _ Cycle(S) _ _ _ (2x) _ _ _ Meeting point(M) _ _ _ {?} _ _ _ Cycle(S) _ _ _</p> </blockquote> <p>We can still follow the same logic and find out the meeting point is at <code class="language-plaintext highlighter-rouge">x + y</code>. And the distance from 2x to the meeting point will be <code class="language-plaintext highlighter-rouge">x + y - 2x</code> = <code class="language-plaintext highlighter-rouge">y - x</code>. With the fact the distance between 2x and Cycle(S) by going forward is <code class="language-plaintext highlighter-rouge">y</code>, Distance (Meeting point to Cycle(S)) will be <code class="language-plaintext highlighter-rouge">y - (y - x)</code> which is <code class="language-plaintext highlighter-rouge">x</code> itself.</p> <p>By knowing <code class="language-plaintext highlighter-rouge">Distance (Head to Cycle start) equals to Distance (Meeting point to Cycle start)</code>, once we detect the tortoise and the hare pointers meet, we can have a pointer starting from the head that moves with a one in the meeting point at the same speed, and the point they meet up will be Cycle(S), the start of the cycle.</p> <h2 id="summary">Summary</h2> <blockquote> <ol> <li> <p>The slow and fast pointers (tortoise and hare) will meet up if there exists a cycle, otherwise the fast pointer will reach a None node.</p> </li> <li> <p>When they reach at the meeting point, one moves from the head with the slow pointer moves from the meeting point at the same speed, the point they meet up is the start of the cycle.</p> </li> </ol> </blockquote> <h2 id="code">Code</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Definition for singly-linked list.
# class ListNode:
#     def __init__(self, x):
#         self.val = x
#         self.next = None
</span>
<span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">detectCycle</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">head</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ListNode</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ListNode</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">head</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">head</span><span class="p">.</span><span class="nb">next</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">None</span>

        <span class="n">slow</span> <span class="o">=</span> <span class="n">head</span><span class="p">.</span><span class="nb">next</span>
        <span class="n">fast</span> <span class="o">=</span> <span class="n">head</span><span class="p">.</span><span class="nb">next</span><span class="p">.</span><span class="nb">next</span>

        <span class="k">while</span> <span class="n">fast</span> <span class="o">!=</span> <span class="n">slow</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">fast</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">fast</span><span class="p">.</span><span class="nb">next</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">None</span>

            <span class="n">slow</span> <span class="o">=</span> <span class="n">slow</span><span class="p">.</span><span class="nb">next</span>
            <span class="n">fast</span> <span class="o">=</span> <span class="n">fast</span><span class="p">.</span><span class="nb">next</span><span class="p">.</span><span class="nb">next</span>

        <span class="n">fast</span> <span class="o">=</span> <span class="n">head</span>
        <span class="k">while</span> <span class="n">slow</span> <span class="o">!=</span> <span class="n">fast</span><span class="p">:</span>
            <span class="n">fast</span> <span class="o">=</span> <span class="n">fast</span><span class="p">.</span><span class="nb">next</span>
            <span class="n">slow</span> <span class="o">=</span> <span class="n">slow</span><span class="p">.</span><span class="nb">next</span>

        <span class="k">return</span> <span class="n">slow</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="leetcode"/><category term="leetcode"/><category term="algorithms"/><category term="solution"/><summary type="html"><![CDATA[A detailed solution to the Linked List Cycle II problem using Floyd's cycle-finding algorithm]]></summary></entry><entry><title type="html">Leetcode 48: Rotate Image</title><link href="https://zyinghua.github.io/blog/2021/leetcode-48/" rel="alternate" type="text/html" title="Leetcode 48: Rotate Image"/><published>2021-12-24T00:00:00+00:00</published><updated>2021-12-24T00:00:00+00:00</updated><id>https://zyinghua.github.io/blog/2021/leetcode-48</id><content type="html" xml:base="https://zyinghua.github.io/blog/2021/leetcode-48/"><![CDATA[<h3 id="code">Code</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Solution</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">rotate</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">matrix</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="sh">"""</span><span class="s">Do not return anything, modify matrix in-place instead.</span><span class="sh">"""</span>
        
        <span class="k">assert</span> <span class="n">matrix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">Matrix is None!</span><span class="sh">"</span>
    
        <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">matrix</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">):</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">row</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span>
                <span class="n">row</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">row</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
    
    
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">)):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">k</span><span class="p">):</span>
                <span class="n">temp</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
                <span class="n">matrix</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">matrix</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">][</span><span class="nf">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">k</span><span class="p">]</span>
                <span class="n">matrix</span><span class="p">[</span><span class="nf">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">i</span><span class="p">][</span><span class="nf">len</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">temp</span>
</code></pre></div></div> <h3 id="first-step-pre-process-each-row">First step: Pre-process each row</h3> <p>Each index swaps with the corresponding index at the end of the row. That is:</p> <p>[0] swaps with [len(row) - 1], [1] swaps with [len(row) - 1 - 1], etc.</p> <p>len(row) // 2 in the code means we only need to swap half-length amount of time, any more swaps will swap back to the original order.</p> <p>An example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1   2   3        3  2  1
4   5   6   -&gt;   6  5  4
7   8   9        9  8  7
</code></pre></div></div> <h3 id="second-step-swap-bits-diagonally">Second step: Swap bits diagonally</h3> <p>Imagine the matrix is composed of multiple sub-matrices, swap each diagonal corner pair (top-left with corresponding bottom-right) that the top-left bit in the pair is LESS than len(matrix) - 1 - k where <code class="language-plaintext highlighter-rouge">k</code> represents the row ID. That is, if you notice the diagonal line starting from top-right to left-bottom, which is a line of bits that are already in their positions by the pre-process (Think about how the rotate works). And then we just need to swap each one counter diagonally once, therefore only swap the bits LESS than the bit in the diagonal line. In the below implementation, <code class="language-plaintext highlighter-rouge">k</code> represents the row ID, <code class="language-plaintext highlighter-rouge">i</code> represents the column ID in that row.</p> <p>An example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3  2  1                     7   2   1                    7   4   1                    7   4   1 
6  5  4   -&gt; 1st k, 1st i:  6   5   4  -&gt; 1st k, 2nd i:  6   5   2  -&gt; 2nd k, 1st i:  8   5   2
9  8  7                     9   8   3                    9   8   3                    9   6   3
</code></pre></div></div>]]></content><author><name></name></author><category term="leetcode"/><category term="leetcode"/><category term="algorithms"/><category term="solution"/><summary type="html"><![CDATA[A solution to the Leetcode problem of rotating an n x n 2D matrix by 90 degrees clockwise]]></summary></entry></feed>